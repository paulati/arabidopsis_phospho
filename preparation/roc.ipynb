{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sys import path\n",
    "base_path = \"/media/paulati/Nuevo vol/paula/ingebi/2020/agustina_mazzella/github/arabidopsis_phospho\"\n",
    "path.append(base_path)    \n",
    "\n",
    "from preparation import util"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import path, remove\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import statistics as stats\n",
    "from sklearn.metrics import roc_curve, roc_auc_score, auc, precision_recall_curve, f1_score, confusion_matrix, recall_score, precision_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "musite_results_base_path = path.join(base_path, 'data/results/musitedeep')\n",
    "musiteDeep_prediction_score_by_id_base_file_path = path.join(musite_results_base_path, \"Prediction_results_score_by_id_base.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "musite_scores_file_path = musiteDeep_prediction_score_by_id_base_file_path.replace(\".txt\", \".zip\")\n",
    "musite_scores_data = pd.read_csv(musite_scores_file_path, sep = \"\\t\")\n",
    "musite_scores_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experimental_file_path = path.join(base_path, \"data/preproc/experimental_ids.zip\")\n",
    "experimental_data = pd.read_csv(experimental_file_path, sep = \"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "musite_scores_data.index = musite_scores_data.protein_id_base\n",
    "experimental_data.index = experimental_data.code\n",
    "musite_experimental_data = musite_scores_data.join(experimental_data, how = \"outer\")\n",
    "#musite_experimental_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "musite_experimental_data[\"label\"] = [1 if x is not np.NaN else 0 for x in musite_experimental_data.code.values]\n",
    "musite_experimental_data[\"color\"] = [\"orange\" if x == 1 else \"blue\" for x in musite_experimental_data.label.values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "musite_experimental_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "musite_experimental_data_sort = musite_experimental_data.sort_values(by=\"min_score\", ascending= True)\n",
    "sns.scatterplot(x = range(musite_experimental_data_sort.shape[0]), y = musite_experimental_data_sort.min_score, \n",
    "                hue = musite_experimental_data_sort.label.values, style = musite_experimental_data_sort.label.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "musite_experimental_data_sort = musite_experimental_data.sort_values(by=\"max_score\", ascending= True)\n",
    "sns.scatterplot(x = range(musite_experimental_data_sort.shape[0]), y = musite_experimental_data_sort.max_score,\n",
    "                hue = musite_experimental_data_sort.label.values, style = musite_experimental_data_sort.label.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "musite_experimental_data_sort = musite_experimental_data.sort_values(by=\"mean_score\", ascending= True)\n",
    "sns.scatterplot(x = range(musite_experimental_data_sort.shape[0]), y = musite_experimental_data_sort.mean_score,\n",
    "                hue = musite_experimental_data_sort.label.values, style = musite_experimental_data_sort.label.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "musite_experimental_data.label.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = musite_experimental_data.label\n",
    "\n",
    "probs = musite_experimental_data.max_score\n",
    "\n",
    "fpr, tpr, thresholds = roc_curve(y, probs)\n",
    "\n",
    "auc = roc_auc_score(y, probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ROC Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate a no skill prediction (majority class)\n",
    "ns_probs = [0 for _ in range(musite_experimental_data.shape[0])]\n",
    "# keep probabilities for the positive outcome only\n",
    "musite_probs = musite_experimental_data.max_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate scores\n",
    "ns_auc = roc_auc_score(musite_experimental_data.label, ns_probs)\n",
    "musite_auc = roc_auc_score(musite_experimental_data.label, musite_probs)\n",
    "# summarize scores\n",
    "print('No Skill: ROC AUC=%.3f' % (ns_auc))\n",
    "print('Logistic: ROC AUC=%.3f' % (musite_auc))\n",
    "# calculate roc curves\n",
    "ns_fpr, ns_tpr, _ = roc_curve(musite_experimental_data.label, ns_probs)\n",
    "musite_fpr, musite_tpr, _ = roc_curve(musite_experimental_data.label, musite_probs)\n",
    "# plot the roc curve for the model\n",
    "plt.plot(ns_fpr, ns_tpr, linestyle='--', label='No Skill')\n",
    "plt.plot(musite_fpr, musite_tpr, marker='.', label='MusteDeep')\n",
    "# axis labels\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "# show the legend\n",
    "plt.legend()\n",
    "# show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimal threshold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.\n",
    "\n",
    "This code evaluates `thresholds_count` samples looking for optimal threshold.\n",
    "\n",
    "`data_frac` specify the proportion of the dataset records evaluated in each sample.\n",
    "\n",
    "Optimal threshold as the maximum `abs(tpr-fpr)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_j_score(tpr, fpr, score_type):\n",
    "    if score_type == 1:\n",
    "        j_scores = abs(tpr-fpr)\n",
    "    elif score_type == 2:\n",
    "        #j_scores = np.sqrt(((1-tpr) * (1-tpr)) + (fpr * fpr))\n",
    "        j_scores = np.sqrt(((1-tpr) * (1-tpr)) + (fpr * fpr))\n",
    "        j_scores = -j_scores\n",
    "    return j_scores\n",
    "\n",
    "    \n",
    "\n",
    "def calculate_optimal_thresholds(pred_exp_data, thresholds_count, data_frac, score_type=1):\n",
    "    \n",
    "    result = []\n",
    "    \n",
    "    exp_pos_mask = pred_exp_data.label == 1\n",
    "    exp_pos = pred_exp_data.loc[exp_pos_mask, :]\n",
    "    prop_exp_pos = exp_pos_mask.sum() / pred_exp_data.shape[0]\n",
    "    \n",
    "    exp_neg_mask = pred_exp_data.label == 0\n",
    "    exp_neg = pred_exp_data.loc[exp_neg_mask, :]\n",
    "    prop_exp_neg = exp_neg_mask.sum() / pred_exp_data.shape[0]\n",
    "    \n",
    "    pos_sample_count = round(data_frac * prop_exp_pos * pred_exp_data.shape[0])\n",
    "    print(pos_sample_count)\n",
    "    \n",
    "    neg_sample_count = round(data_frac * prop_exp_neg * pred_exp_data.shape[0])\n",
    "    print(neg_sample_count)\n",
    "    \n",
    "    for i in range(thresholds_count):\n",
    "    \n",
    "        sample_pos = exp_pos.sample(pos_sample_count, replace = True, axis = 0 )\n",
    "        \n",
    "        #print(sample_pos.shape[0])\n",
    "        \n",
    "        sample_neg = exp_neg.sample(neg_sample_count, replace = True, axis = 0 )\n",
    "        \n",
    "        #print(sample_neg.shape[0])\n",
    "    \n",
    "        sample = pd.concat([sample_pos, sample_neg], axis = 0)\n",
    "        \n",
    "        #print(sample.shape[0])\n",
    "    \n",
    "        y = pd.Categorical(sample.label.values)\n",
    "    \n",
    "        scores = sample.max_score.values\n",
    "    \n",
    "        fpr, tpr, thresholds = roc_curve(y, scores, pos_label=1)\n",
    "    #roc_auc = metrics.auc(fpr, tpr)\n",
    "    \n",
    "        #j_scores = abs(tpr-fpr)\n",
    "        j_scores = calculate_j_score(tpr, fpr, score_type)        \n",
    "    \n",
    "        optimal_idx = np.argmax(j_scores)\n",
    "    \n",
    "        optimal_threshold = thresholds[optimal_idx]\n",
    "        \n",
    "        result.append(optimal_threshold)\n",
    "    \n",
    "    \n",
    "    return(result)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimal_thresholds = calculate_optimal_thresholds(musite_experimental_data, 5000, 0.3, 1)\n",
    "print(np.mean(optimal_thresholds))\n",
    "print(np.std(optimal_thresholds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(optimal_thresholds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimal_1 = stats.mode(optimal_thresholds)\n",
    "optimal_1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Without sampling:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimal_thresholds = calculate_optimal_thresholds(musite_experimental_data, 1, 1, 1)\n",
    "optimal_thresholds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.\n",
    "\n",
    "Pick the threshold t∗ that minimizes the distance to the top left can be done by measuring its distance \n",
    "\n",
    "$distance = (0, 1) - (fpr, tpr)$\n",
    "\n",
    "$= (-fpr, 1-tpr)$\n",
    "\n",
    "$= \\sqrt{(-fpr)^2 + (1-tpr)^2}$\n",
    "\n",
    "\n",
    "$t∗ = argmin_t(\\sqrt{fpr_t^2 + (1 - tpr_t)^2}$\n",
    "\n",
    "$t∗ = argmax_t(- \\sqrt{fpr_t^2 + (1 - tpr_t)^2}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimal_thresholds = calculate_optimal_thresholds(musite_experimental_data, 5000, 0.3, 2)\n",
    "print(np.mean(optimal_thresholds))\n",
    "print(np.std(optimal_thresholds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(optimal_thresholds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimal_2 = stats.mode(optimal_thresholds)\n",
    "optimal_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimal_thresholds = calculate_optimal_thresholds(musite_experimental_data, 1, 1, 2)\n",
    "optimal_thresholds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = musite_experimental_data.label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimal_threshold_1 = optimal_1\n",
    "y_pred = [1 if x >= optimal_threshold_1 else 0 for x in musite_experimental_data.max_score.values]\n",
    "confusion_matrix(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimal_threshold_2 = optimal_2\n",
    "y_pred = [1 if x >= optimal_threshold_2 else 0 for x in musite_experimental_data.max_score.values]\n",
    "confusion_matrix(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "default_threshold = 0.5\n",
    "y_pred = [1 if x >= default_threshold else 0 for x in musite_experimental_data.max_score.values]\n",
    "confusion_matrix(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Threshold is set in 0.852**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Precision-Recall\n",
    "\n",
    "<img src='./img/800px-Precisionrecall.svg.png' width= 20%></img>\n",
    "\n",
    "source: https://en.wikipedia.org/wiki/Precision_and_recall#/media/File:Precisionrecall.svg\n",
    "\n",
    "The precision-recall curve shows the tradeoff between precision and recall for different threshold. \n",
    "\n",
    "A high area under the curve represents both high recall and high precision, where high precision relates to a low false positive rate, and high recall relates to a low false negative rate. \n",
    "\n",
    "High scores for both show that the classifier is returning accurate results (high precision), as well as returning a majority of all positive results (high recall).\n",
    "\n",
    "source: https://scikit-learn.org/stable/auto_examples/model_selection/plot_precision_recall.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "musite_precision, musite_recall, _ = precision_recall_curve(musite_experimental_data.label, musite_probs)\n",
    "# calculate scores\n",
    "\n",
    "y_pred = [1 if x >= optimal_1 else 0 for x in musite_experimental_data.max_score.values]\n",
    "\n",
    "musite_f1 = f1_score(musite_experimental_data.label, y_pred)\n",
    "\n",
    "musite_auc = auc(musite_recall, musite_precision)\n",
    "# summarize scores\n",
    "print('MusiteDeep: f1=%.3f auc=%.3f' % (musite_f1, musite_auc))\n",
    "# plot the precision-recall curves\n",
    "no_skill = len(musite_experimental_data.label == 1) / len(musite_experimental_data.label)\n",
    "plt.plot([0, 1], [no_skill, no_skill], linestyle='--', label='Null Model')\n",
    "plt.plot(musite_recall, musite_precision, marker='.', label='MusiteDeep')\n",
    "# axis labels\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "# show the legend\n",
    "plt.legend()\n",
    "# show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold_count = 100\n",
    "thresholds = []\n",
    "for i in range(threshold_count):\n",
    "    threshold = i / thr_count\n",
    "    thresholds.append(threshold)\n",
    "\n",
    "f1_scores = []\n",
    "recall_scores = []\n",
    "precision_scores = []\n",
    "y_true = musite_experimental_data.label\n",
    "    \n",
    "for threshold in thresholds:\n",
    "    y_pred = [1 if x >= threshold else 0 for x in musite_experimental_data.max_score.values]\n",
    "    f1 = f1_score(y_true, y_pred)    \n",
    "    recall = recall_score(y_true, y_pred)    \n",
    "    precision = precision_score(y_true, y_pred)    \n",
    "    f1_scores.append(f1)\n",
    "    recall_scores.append(recall)\n",
    "    precision_scores.append(precision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(precision_scores, color=\"steelblue\")\n",
    "sns.histplot(recall_scores, color=\"orange\")\n",
    "sns.histplot(f1_scores, color = \"darkred\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(x = thresholds, y = f1_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max(f1_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = np.argmax(f1_scores)\n",
    "print(recall_scores[index])\n",
    "print(precision_scores[index])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Metrics with optimal_1 threshold :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = [1 if x >= optimal_1 else 0 for x in musite_experimental_data.max_score.values]\n",
    "\n",
    "f1 = f1_score(y_true, y_pred)    \n",
    "recall = recall_score(y_true, y_pred)    \n",
    "precision = precision_score(y_true, y_pred)    \n",
    "\n",
    "print(f1)\n",
    "print(recall)\n",
    "print(precision)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "---\n",
    "\n",
    "https://machinelearningmastery.com/roc-curves-and-precision-recall-curves-for-classification-in-python/\n",
    "\n",
    "https://stats.stackexchange.com/questions/123124/how-to-determine-the-optimal-threshold-for-a-classifier-and-generate-roc-curve\n",
    "\n",
    "http://www0.cs.ucl.ac.uk/staff/W.Langdon/roc/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimal_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
